{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../../requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../../requirements.txt (line 2)) (0.4.0a0+6b959ee)\n",
      "Requirement already satisfied: dominate>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../../requirements.txt (line 3)) (2.4.0)\n",
      "Requirement already satisfied: visdom>=0.1.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../../requirements.txt (line 4)) (0.1.8.9)\n",
      "Requirement already satisfied: werkzeug in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../../requirements.txt (line 5)) (0.14.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=0.4.1->-r ../../../requirements.txt (line 1)) (1.15.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision>=0.2.1->-r ../../../requirements.txt (line 2)) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision>=0.2.1->-r ../../../requirements.txt (line 2)) (5.2.0)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: jsonpatch in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (1.24)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (2.20.0)\n",
      "Requirement already satisfied: websocket-client in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (0.56.0)\n",
      "Requirement already satisfied: torchfile in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (0.1.0)\n",
      "Requirement already satisfied: tornado in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (5.0.2)\n",
      "Requirement already satisfied: pyzmq in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (17.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from jsonpatch->visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->visdom>=0.1.8.3->-r ../../../requirements.txt (line 4)) (1.23)\n",
      "\u001b[31mfastai 1.0.59 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../..')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "import util.util as util\n",
    "import os, time\n",
    "from train import train\n",
    "from test import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 4                             \t[default: 1]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./model-checkpoints           \t[default: ./checkpoints]\n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/van-gogh           \t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 1000                          \t[default: 400]\n",
      "               display_id: -1                            \t[default: 1]\n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "               n_layers_D: 3                             \n",
      "                     name: van-gogh                      \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "                    niter: 100                           \n",
      "              niter_decay: 100                           \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: crop                          \t[default: resize_and_crop]\n",
      "               print_freq: 1000                          \t[default: 100]\n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 10000                         \t[default: 5]\n",
      "         save_latest_freq: 1000                          \t[default: 5000]\n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "The number of training images = 235\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "create web directory ./model-checkpoints/van-gogh/web...\n",
      "End of epoch 1 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 2 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 3 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 4 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 5, iters: 56, time: 0.254, data: 0.915) D_A: 0.371 G_A: 0.508 cycle_A: 3.153 idt_A: 1.420 D_B: 0.203 G_B: 0.311 cycle_B: 2.893 idt_B: 1.458 \n",
      "saving the latest model (epoch 5, total_iters 1000)\n",
      "End of epoch 5 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 6 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 7 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 8 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 9, iters: 112, time: 0.202, data: 0.002) D_A: 0.307 G_A: 0.455 cycle_A: 3.057 idt_A: 1.844 D_B: 0.191 G_B: 0.293 cycle_B: 3.777 idt_B: 1.458 \n",
      "saving the latest model (epoch 9, total_iters 2000)\n",
      "End of epoch 9 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 10 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 11 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 12 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 13, iters: 168, time: 0.210, data: 0.003) D_A: 0.210 G_A: 0.564 cycle_A: 5.523 idt_A: 1.334 D_B: 0.184 G_B: 0.352 cycle_B: 2.999 idt_B: 2.157 \n",
      "saving the latest model (epoch 13, total_iters 3000)\n",
      "End of epoch 13 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 14 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 15 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 16 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 17, iters: 224, time: 0.205, data: 0.003) D_A: 0.413 G_A: 0.533 cycle_A: 1.734 idt_A: 1.021 D_B: 0.270 G_B: 0.258 cycle_B: 2.227 idt_B: 0.872 \n",
      "saving the latest model (epoch 17, total_iters 4000)\n",
      "End of epoch 17 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 18 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 19 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 20 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 21 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 22, iters: 44, time: 0.218, data: 0.002) D_A: 0.218 G_A: 0.349 cycle_A: 2.654 idt_A: 1.223 D_B: 0.384 G_B: 0.818 cycle_B: 2.869 idt_B: 1.087 \n",
      "saving the latest model (epoch 22, total_iters 5000)\n",
      "End of epoch 22 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 23 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 24 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 25 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 26, iters: 100, time: 0.230, data: 0.003) D_A: 0.186 G_A: 0.531 cycle_A: 4.343 idt_A: 1.396 D_B: 0.220 G_B: 0.226 cycle_B: 3.263 idt_B: 2.312 \n",
      "saving the latest model (epoch 26, total_iters 6000)\n",
      "End of epoch 26 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 27 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 28 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 29 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 30, iters: 156, time: 0.258, data: 0.002) D_A: 0.276 G_A: 0.638 cycle_A: 3.012 idt_A: 0.984 D_B: 0.208 G_B: 0.286 cycle_B: 2.272 idt_B: 1.519 \n",
      "saving the latest model (epoch 30, total_iters 7000)\n",
      "End of epoch 30 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 31 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 32 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 33 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 34, iters: 212, time: 0.232, data: 0.002) D_A: 0.091 G_A: 0.446 cycle_A: 2.895 idt_A: 1.107 D_B: 0.532 G_B: 0.532 cycle_B: 2.189 idt_B: 1.403 \n",
      "saving the latest model (epoch 34, total_iters 8000)\n",
      "End of epoch 34 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 35 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 36 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 37 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 38 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 39, iters: 32, time: 0.247, data: 0.003) D_A: 0.206 G_A: 0.558 cycle_A: 2.842 idt_A: 0.884 D_B: 0.218 G_B: 0.264 cycle_B: 1.937 idt_B: 1.258 \n",
      "saving the latest model (epoch 39, total_iters 9000)\n",
      "End of epoch 39 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 40 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 41 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 42 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 43, iters: 88, time: 0.251, data: 0.003) D_A: 0.212 G_A: 0.264 cycle_A: 2.538 idt_A: 0.800 D_B: 0.128 G_B: 0.672 cycle_B: 1.821 idt_B: 1.054 \n",
      "saving the latest model (epoch 43, total_iters 10000)\n",
      "End of epoch 43 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 44 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 45 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 46 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 47, iters: 144, time: 0.256, data: 0.003) D_A: 0.261 G_A: 0.346 cycle_A: 3.898 idt_A: 0.961 D_B: 0.155 G_B: 0.443 cycle_B: 2.142 idt_B: 1.227 \n",
      "saving the latest model (epoch 47, total_iters 11000)\n",
      "End of epoch 47 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 48 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 49 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 50 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 51, iters: 200, time: 0.267, data: 0.003) D_A: 0.139 G_A: 0.261 cycle_A: 2.301 idt_A: 0.937 D_B: 0.157 G_B: 0.733 cycle_B: 2.237 idt_B: 1.099 \n",
      "saving the latest model (epoch 51, total_iters 12000)\n",
      "End of epoch 51 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 52 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 53 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 54 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 55 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 56, iters: 20, time: 0.272, data: 0.002) D_A: 0.146 G_A: 0.390 cycle_A: 2.415 idt_A: 0.780 D_B: 0.244 G_B: 0.859 cycle_B: 1.623 idt_B: 1.057 \n",
      "saving the latest model (epoch 56, total_iters 13000)\n",
      "End of epoch 56 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 57 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 58 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 59 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 60, iters: 76, time: 0.315, data: 0.001) D_A: 0.215 G_A: 0.223 cycle_A: 2.071 idt_A: 0.893 D_B: 0.145 G_B: 0.592 cycle_B: 1.933 idt_B: 0.910 \n",
      "saving the latest model (epoch 60, total_iters 14000)\n",
      "End of epoch 60 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 61 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 62 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 63 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 64, iters: 132, time: 0.295, data: 0.002) D_A: 0.069 G_A: 0.265 cycle_A: 2.290 idt_A: 0.854 D_B: 0.141 G_B: 0.439 cycle_B: 2.070 idt_B: 1.054 \n",
      "saving the latest model (epoch 64, total_iters 15000)\n",
      "End of epoch 64 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 65 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 66 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 67 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 68, iters: 188, time: 0.289, data: 0.003) D_A: 0.101 G_A: 0.444 cycle_A: 1.824 idt_A: 0.925 D_B: 0.089 G_B: 0.613 cycle_B: 2.041 idt_B: 0.835 \n",
      "saving the latest model (epoch 68, total_iters 16000)\n",
      "End of epoch 68 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 69 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 70 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 71 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 72 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 73, iters: 8, time: 0.302, data: 0.002) D_A: 0.138 G_A: 0.424 cycle_A: 1.880 idt_A: 0.932 D_B: 0.113 G_B: 0.627 cycle_B: 1.884 idt_B: 0.974 \n",
      "saving the latest model (epoch 73, total_iters 17000)\n",
      "End of epoch 73 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 74 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 75 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 76 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 77, iters: 64, time: 0.303, data: 0.001) D_A: 0.200 G_A: 0.376 cycle_A: 3.032 idt_A: 0.776 D_B: 0.142 G_B: 0.203 cycle_B: 1.803 idt_B: 1.370 \n",
      "saving the latest model (epoch 77, total_iters 18000)\n",
      "End of epoch 77 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 78 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 79 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 80 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 81, iters: 120, time: 0.346, data: 0.002) D_A: 0.124 G_A: 0.534 cycle_A: 1.655 idt_A: 1.129 D_B: 0.201 G_B: 0.176 cycle_B: 2.310 idt_B: 0.758 \n",
      "saving the latest model (epoch 81, total_iters 19000)\n",
      "End of epoch 81 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 82 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 83 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 84 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 85, iters: 176, time: 0.335, data: 0.002) D_A: 0.206 G_A: 0.174 cycle_A: 2.292 idt_A: 0.855 D_B: 0.236 G_B: 0.169 cycle_B: 1.916 idt_B: 1.198 \n",
      "saving the latest model (epoch 85, total_iters 20000)\n",
      "End of epoch 85 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 86 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 87 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 88 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 89, iters: 232, time: 0.317, data: 0.003) D_A: 0.121 G_A: 0.352 cycle_A: 1.831 idt_A: 1.184 D_B: 0.102 G_B: 0.299 cycle_B: 2.258 idt_B: 0.790 \n",
      "saving the latest model (epoch 89, total_iters 21000)\n",
      "End of epoch 89 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 90 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 91 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 92 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 93 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 94, iters: 52, time: 0.364, data: 0.002) D_A: 0.201 G_A: 0.745 cycle_A: 1.846 idt_A: 0.744 D_B: 0.071 G_B: 0.443 cycle_B: 1.759 idt_B: 0.894 \n",
      "saving the latest model (epoch 94, total_iters 22000)\n",
      "End of epoch 94 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 95 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 96 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 97 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "(epoch: 98, iters: 108, time: 0.334, data: 0.003) D_A: 0.134 G_A: 0.450 cycle_A: 2.298 idt_A: 0.869 D_B: 0.127 G_B: 0.608 cycle_B: 1.726 idt_B: 1.108 \n",
      "saving the latest model (epoch 98, total_iters 23000)\n",
      "End of epoch 98 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 99 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0002000\n",
      "End of epoch 100 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001980\n",
      "End of epoch 101 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001960\n",
      "(epoch: 102, iters: 164, time: 0.346, data: 0.003) D_A: 0.280 G_A: 0.564 cycle_A: 1.332 idt_A: 0.885 D_B: 0.171 G_B: 0.488 cycle_B: 1.877 idt_B: 0.688 \n",
      "saving the latest model (epoch 102, total_iters 24000)\n",
      "End of epoch 102 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0001941\n",
      "End of epoch 103 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001921\n",
      "End of epoch 104 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001901\n",
      "End of epoch 105 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001881\n",
      "(epoch: 106, iters: 220, time: 0.384, data: 0.003) D_A: 0.339 G_A: 0.139 cycle_A: 1.599 idt_A: 0.779 D_B: 0.292 G_B: 0.551 cycle_B: 1.481 idt_B: 0.796 \n",
      "saving the latest model (epoch 106, total_iters 25000)\n",
      "End of epoch 106 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0001861\n",
      "End of epoch 107 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001842\n",
      "End of epoch 108 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001822\n",
      "End of epoch 109 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001802\n",
      "End of epoch 110 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001782\n",
      "(epoch: 111, iters: 40, time: 0.361, data: 0.003) D_A: 0.159 G_A: 0.507 cycle_A: 1.631 idt_A: 0.595 D_B: 0.173 G_B: 0.850 cycle_B: 1.361 idt_B: 0.791 \n",
      "saving the latest model (epoch 111, total_iters 26000)\n",
      "End of epoch 111 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0001762\n",
      "End of epoch 112 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001743\n",
      "End of epoch 113 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001723\n",
      "End of epoch 114 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001703\n",
      "(epoch: 115, iters: 96, time: 0.347, data: 0.002) D_A: 0.108 G_A: 0.567 cycle_A: 4.273 idt_A: 1.347 D_B: 0.122 G_B: 0.309 cycle_B: 2.024 idt_B: 2.002 \n",
      "saving the latest model (epoch 115, total_iters 27000)\n",
      "End of epoch 115 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0001683\n",
      "End of epoch 116 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001663\n",
      "End of epoch 117 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001644\n",
      "End of epoch 118 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001624\n",
      "(epoch: 119, iters: 152, time: 0.393, data: 0.002) D_A: 0.087 G_A: 0.610 cycle_A: 1.989 idt_A: 0.676 D_B: 0.078 G_B: 0.249 cycle_B: 1.391 idt_B: 0.950 \n",
      "saving the latest model (epoch 119, total_iters 28000)\n",
      "End of epoch 119 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0001604\n",
      "End of epoch 120 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001584\n",
      "End of epoch 121 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001564\n",
      "End of epoch 122 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001545\n",
      "(epoch: 123, iters: 208, time: 0.366, data: 0.002) D_A: 0.229 G_A: 0.404 cycle_A: 1.901 idt_A: 0.546 D_B: 0.184 G_B: 0.156 cycle_B: 1.272 idt_B: 0.842 \n",
      "saving the latest model (epoch 123, total_iters 29000)\n",
      "End of epoch 123 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0001525\n",
      "End of epoch 124 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001505\n",
      "End of epoch 125 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001485\n",
      "End of epoch 126 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001465\n",
      "End of epoch 127 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001446\n",
      "(epoch: 128, iters: 28, time: 0.381, data: 0.002) D_A: 0.184 G_A: 0.585 cycle_A: 1.383 idt_A: 0.578 D_B: 0.053 G_B: 0.445 cycle_B: 1.448 idt_B: 0.843 \n",
      "saving the latest model (epoch 128, total_iters 30000)\n",
      "End of epoch 128 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0001426\n",
      "End of epoch 129 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001406\n",
      "End of epoch 130 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001386\n",
      "End of epoch 131 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001366\n",
      "(epoch: 132, iters: 84, time: 0.413, data: 0.001) D_A: 0.137 G_A: 0.422 cycle_A: 1.676 idt_A: 0.693 D_B: 0.093 G_B: 0.440 cycle_B: 1.543 idt_B: 0.838 \n",
      "saving the latest model (epoch 132, total_iters 31000)\n",
      "End of epoch 132 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0001347\n",
      "End of epoch 133 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001327\n",
      "End of epoch 134 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001307\n",
      "End of epoch 135 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001287\n",
      "(epoch: 136, iters: 140, time: 0.390, data: 0.003) D_A: 0.180 G_A: 0.291 cycle_A: 2.184 idt_A: 0.749 D_B: 0.052 G_B: 0.380 cycle_B: 1.716 idt_B: 1.031 \n",
      "saving the latest model (epoch 136, total_iters 32000)\n",
      "End of epoch 136 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0001267\n",
      "End of epoch 137 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001248\n",
      "End of epoch 138 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001228\n",
      "End of epoch 139 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001208\n",
      "(epoch: 140, iters: 196, time: 0.436, data: 0.003) D_A: 0.198 G_A: 0.474 cycle_A: 2.484 idt_A: 0.934 D_B: 0.066 G_B: 0.747 cycle_B: 1.807 idt_B: 0.968 \n",
      "saving the latest model (epoch 140, total_iters 33000)\n",
      "End of epoch 140 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0001188\n",
      "End of epoch 141 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001168\n",
      "End of epoch 142 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001149\n",
      "End of epoch 143 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001129\n",
      "End of epoch 144 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001109\n",
      "(epoch: 145, iters: 16, time: 0.417, data: 0.003) D_A: 0.152 G_A: 0.476 cycle_A: 1.862 idt_A: 0.722 D_B: 0.084 G_B: 0.457 cycle_B: 1.508 idt_B: 0.944 \n",
      "saving the latest model (epoch 145, total_iters 34000)\n",
      "End of epoch 145 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0001089\n",
      "End of epoch 146 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001069\n",
      "End of epoch 147 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001050\n",
      "End of epoch 148 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0001030\n",
      "(epoch: 149, iters: 72, time: 0.452, data: 0.003) D_A: 0.205 G_A: 0.309 cycle_A: 1.899 idt_A: 0.614 D_B: 0.111 G_B: 0.418 cycle_B: 1.189 idt_B: 0.918 \n",
      "saving the latest model (epoch 149, total_iters 35000)\n",
      "End of epoch 149 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0001010\n",
      "End of epoch 150 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000990\n",
      "End of epoch 151 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000970\n",
      "End of epoch 152 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000950\n",
      "(epoch: 153, iters: 128, time: 0.406, data: 0.002) D_A: 0.125 G_A: 0.362 cycle_A: 2.511 idt_A: 0.641 D_B: 0.104 G_B: 0.522 cycle_B: 1.449 idt_B: 0.928 \n",
      "saving the latest model (epoch 153, total_iters 36000)\n",
      "End of epoch 153 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000931\n",
      "End of epoch 154 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000911\n",
      "End of epoch 155 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000891\n",
      "End of epoch 156 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000871\n",
      "(epoch: 157, iters: 184, time: 0.459, data: 0.003) D_A: 0.126 G_A: 0.650 cycle_A: 1.934 idt_A: 0.575 D_B: 0.040 G_B: 0.540 cycle_B: 1.295 idt_B: 0.714 \n",
      "saving the latest model (epoch 157, total_iters 37000)\n",
      "End of epoch 157 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000851\n",
      "End of epoch 158 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000832\n",
      "End of epoch 159 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000812\n",
      "End of epoch 160 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000792\n",
      "End of epoch 161 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000772\n",
      "(epoch: 162, iters: 4, time: 0.440, data: 0.003) D_A: 0.213 G_A: 0.766 cycle_A: 2.162 idt_A: 0.774 D_B: 0.114 G_B: 0.236 cycle_B: 1.759 idt_B: 1.126 \n",
      "saving the latest model (epoch 162, total_iters 38000)\n",
      "End of epoch 162 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000752\n",
      "End of epoch 163 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000733\n",
      "End of epoch 164 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000713\n",
      "End of epoch 165 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000693\n",
      "(epoch: 166, iters: 60, time: 0.472, data: 0.003) D_A: 0.160 G_A: 0.494 cycle_A: 1.276 idt_A: 0.574 D_B: 0.243 G_B: 0.627 cycle_B: 1.370 idt_B: 0.608 \n",
      "saving the latest model (epoch 166, total_iters 39000)\n",
      "End of epoch 166 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000673\n",
      "End of epoch 167 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000653\n",
      "End of epoch 168 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000634\n",
      "End of epoch 169 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000614\n",
      "(epoch: 170, iters: 116, time: 0.445, data: 0.003) D_A: 0.180 G_A: 0.398 cycle_A: 1.181 idt_A: 0.614 D_B: 0.136 G_B: 0.454 cycle_B: 1.259 idt_B: 0.680 \n",
      "saving the latest model (epoch 170, total_iters 40000)\n",
      "End of epoch 170 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000594\n",
      "End of epoch 171 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000574\n",
      "End of epoch 172 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000554\n",
      "End of epoch 173 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000535\n",
      "(epoch: 174, iters: 172, time: 0.497, data: 0.002) D_A: 0.209 G_A: 0.314 cycle_A: 1.872 idt_A: 0.606 D_B: 0.095 G_B: 0.445 cycle_B: 1.250 idt_B: 1.026 \n",
      "saving the latest model (epoch 174, total_iters 41000)\n",
      "End of epoch 174 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000515\n",
      "End of epoch 175 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000495\n",
      "End of epoch 176 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000475\n",
      "End of epoch 177 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000455\n",
      "(epoch: 178, iters: 228, time: 0.468, data: 0.003) D_A: 0.254 G_A: 0.430 cycle_A: 1.322 idt_A: 0.587 D_B: 0.108 G_B: 0.615 cycle_B: 1.412 idt_B: 0.524 \n",
      "saving the latest model (epoch 178, total_iters 42000)\n",
      "End of epoch 178 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000436\n",
      "End of epoch 179 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000416\n",
      "End of epoch 180 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000396\n",
      "End of epoch 181 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000376\n",
      "End of epoch 182 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000356\n",
      "(epoch: 183, iters: 48, time: 0.602, data: 0.002) D_A: 0.149 G_A: 0.324 cycle_A: 1.287 idt_A: 0.751 D_B: 0.088 G_B: 0.424 cycle_B: 1.478 idt_B: 0.616 \n",
      "saving the latest model (epoch 183, total_iters 43000)\n",
      "End of epoch 183 / 200 \t Time Taken: 38 sec\n",
      "learning rate = 0.0000337\n",
      "End of epoch 184 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000317\n",
      "End of epoch 185 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000297\n",
      "End of epoch 186 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000277\n",
      "(epoch: 187, iters: 104, time: 0.501, data: 0.003) D_A: 0.167 G_A: 0.453 cycle_A: 1.894 idt_A: 0.713 D_B: 0.171 G_B: 0.696 cycle_B: 1.243 idt_B: 0.718 \n",
      "saving the latest model (epoch 187, total_iters 44000)\n",
      "End of epoch 187 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000257\n",
      "End of epoch 188 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000238\n",
      "End of epoch 189 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000218\n",
      "End of epoch 190 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000198\n",
      "(epoch: 191, iters: 160, time: 0.488, data: 0.003) D_A: 0.157 G_A: 0.565 cycle_A: 1.919 idt_A: 0.690 D_B: 0.108 G_B: 0.459 cycle_B: 1.337 idt_B: 0.780 \n",
      "saving the latest model (epoch 191, total_iters 45000)\n",
      "End of epoch 191 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000178\n",
      "End of epoch 192 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000158\n",
      "End of epoch 193 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000139\n",
      "End of epoch 194 / 200 \t Time Taken: 36 sec\n",
      "learning rate = 0.0000119\n",
      "(epoch: 195, iters: 216, time: 0.521, data: 0.002) D_A: 0.168 G_A: 0.367 cycle_A: 1.565 idt_A: 0.697 D_B: 0.219 G_B: 0.512 cycle_B: 1.531 idt_B: 0.925 \n",
      "saving the latest model (epoch 195, total_iters 46000)\n",
      "End of epoch 195 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000099\n",
      "End of epoch 196 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000079\n",
      "End of epoch 197 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000059\n",
      "End of epoch 198 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000040\n",
      "End of epoch 199 / 200 \t Time Taken: 35 sec\n",
      "learning rate = 0.0000020\n",
      "(epoch: 200, iters: 36, time: 0.500, data: 0.002) D_A: 0.178 G_A: 0.401 cycle_A: 1.802 idt_A: 0.669 D_B: 0.121 G_B: 0.429 cycle_B: 1.251 idt_B: 0.700 \n",
      "saving the latest model (epoch 200, total_iters 47000)\n",
      "End of epoch 200 / 200 \t Time Taken: 37 sec\n",
      "learning rate = 0.0000000\n"
     ]
    }
   ],
   "source": [
    "train(['--dataroot=data','--name=baseline','--checkpoints_dir=model-checkpoints','--update_html_freq=1000','--print_freq=1000','--display_freq=1000','--display_id=-1','--save_latest_freq=1000','--num_threads=4','--batch_size=4', '--preprocess=crop', '--crop_size=256', '--save_epoch_freq=10000'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
